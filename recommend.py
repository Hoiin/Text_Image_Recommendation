# -*- coding: utf-8 -*-
"""Recommend.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ovyTZtiKIwsA3yxQeRcWg2ddJljIqBct

# Package
"""

!pip install mxnet
!pip install gluonnlp tqdm
!pip install sentencepiece
!pip install transformers
!pip install soynlp
!pip install emoji
!pip install AdamP

import os
import pandas as pd
import numpy as np
from tqdm.notebook import tqdm
import matplotlib.pyplot as plt
import matplotlib.image as img

import torch
from torch.nn import functional as F
from torch.utils.data import DataLoader, Dataset
from torch.utils.data import TensorDataset, RandomSampler, SequentialSampler
from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, BertConfig

import tensorflow as tf
from keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing import image

from sklearn.model_selection import train_test_split
from keras.preprocessing.sequence import pad_sequences

import cv2

"""# 텍스트 모델 불러오기

## Model Initialization
"""

#GPU 설정
cuda = torch.device('cuda')

#모델 Initialization
#"beomi/KcELECTRA-base"
#"beomi/kcbert-base"
tokenizer = AutoTokenizer.from_pretrained("beomi/kcbert-base", do_lower_case=False)

config = BertConfig.from_pretrained('beomi/kcbert-base')
config.num_labels = 6
model = AutoModelForSequenceClassification.from_pretrained("beomi/kcbert-base",
                                                         config = config).to(cuda)

#학습시켜 놓은 모델 설정
file_path = "/content/drive/MyDrive/비타민 컨퍼런스/Model/kcbert_3"
model.load_state_dict(torch.load(file_path))
model.to(cuda)

"""## Text Preprocessor"""

# 입력 데이터 변환
def convert_input_data(sentences):
    global tokenizer

    # BERT의 토크나이저로 문장을 토큰으로 분리
    tokenized_texts = tokenizer.tokenize(sentences)

    # 입력 토큰의 최대 시퀀스 길이
    MAX_LEN = 128

    # 토큰을 숫자 인덱스로 변환
    input_ids = [[tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]]
    
    # 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움
    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype="long", truncating="post", padding="post")

    # 어텐션 마스크 초기화
    attention_masks = []

    # 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정
    # 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상
    for seq in input_ids:
        seq_mask = [float(i>0) for i in seq]
        attention_masks.append(seq_mask)

    # 데이터를 파이토치의 텐서로 변환
    inputs = torch.tensor(input_ids)
    masks = torch.tensor(attention_masks)

    return inputs, masks

def logits_to_softmax(logits):
  odds = np.exp(logits)
  total = odds.sum()
  softmax = odds/total
  return softmax

def classify_sentence(new_sentence):
  model.eval()

  inputs, masks = convert_input_data(new_sentence)
  b_input_ids = inputs.to(cuda)
  b_input_mask = masks.to(cuda)

  with torch.no_grad():     
    # Forward 수행
    outputs = model(b_input_ids, 
                    token_type_ids=None, 
                    attention_mask=b_input_mask)

  logits = outputs[0]
  logits = logits.detach().cpu().numpy()[0]

  result = logits_to_softmax(logits)

  emotion_dict = {0:"angry",1:"disgust",2:"fear",3:"happy",4:"sad",5:"surprise"}

  for i in range(len(result)):
    print(f"{emotion_dict[i]} : {round(result[i]*100,3)}%")
  
  return result

new_sentence = "아 개웃기다ㅋㅋ"
classify_sentence(new_sentence)

"""# 이미지 모델 불러오기

## Face Detection
"""

cascade_filename = '/content/drive/MyDrive/비타민 컨퍼런스/haarcascade_frontalface_alt.xml'
cascade = cv2.CascadeClassifier(cascade_filename)

"""## Image Model Initialization"""

img_model = tf.keras.models.load_model('/content/drive/MyDrive/비타민 컨퍼런스/Model/CNN_7.h5')

# 사진 출력
def imgDetector(img,cascade,model):
    img = cv2.resize(img,dsize=None,fx=0.5,fy=0.5)
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    results = cascade.detectMultiScale(gray,
                                  scaleFactor = 1.2,
                                  minNeighbors = 1,
                                  minSize=(10,10)
                                  )
    max_area = 0
    area = 0
    
    if len(results) >0:
        for box in results:
            x,y,w,h = box
            cv2.rectangle(img, (x,y), (x+w,y+h), (255,0,0), thickness=1)
            area = h*w

            if area > max_area:
                max_area = area
                face = img[y:y+h,x:x+h]

        #cv2.imwrite('roi.jpg',face)
        output_face = cv2.resize(face,dsize=(48,48),fx=0.5,fy=0.5)
        gray_output_face = cv2.cvtColor(output_face, cv2.COLOR_BGR2GRAY)
        # plt.imshow(gray_output_face,cmap='gray')
        # print("Face is detected!")
        img_array = image.img_to_array(gray_output_face)/255.0
        face_resized = tf.reshape(img_array, (-1,48,48,1))
        # face_resized
        pred = model.predict(face_resized,steps=1)[0]
        
        emotion_dict = {0:"angry",1:"disgust",2:"fearful",3:"happy",4:"sad",5:"surprised"}
        # for i in range(len(pred)):
            # print(f"{emotion_dict[i]} : {round(pred[i]*100,2)}%")
        return pred
        
    else:
        output_img = cv2.resize(img,dsize=(48,48),fx=0.5,fy=0.5)
        gray_output_img = cv2.cvtColor(output_img, cv2.COLOR_BGR2GRAY)
        plt.imshow(gray_output_img,cmap='gray')
        # print(f"There is no detected face : {img}")
        return

base_dir = '/content/drive/MyDrive/비타민 컨퍼런스'
img_dir = os.path.join(base_dir, 'images')
all_img_fnames = os.listdir( img_dir )

img_result = {}
total_not_detected = 0

for i in all_img_fnames : 
    try:
      img_read = cv2.imread(img_dir + f'/{i}')
      face_dict = imgDetector(img_read,cascade,img_model)

      if face_dict is None:
        total_not_detected += 1
        print(f"No face detected : {i}")

      img_result[i] = face_dict

    except:
      print(f"Error image : {i}")
      pass

print(f"Number of not detected img : {total_not_detected}")

img_result = pd.DataFrame(img_result)
img_result = img_result.rename(index = {0:"angry",1:"disgust",2:"fearful",3:"happy",4:"sad",5:"surprised"} ) 
img_result = img_result.dropna(axis = 1 )

img_result.to_csv("/content/drive/MyDrive/비타민 컨퍼런스/CNN_7_result",index = False)

"""# Recommend"""

img_result = pd.read_csv("/content/drive/MyDrive/비타민 컨퍼런스/CNN_7_result")

def recommend_images(sentence) : 
    global img_result
    res = classify_sentence(sentence)
    img_result['sentence'] = res

    def cos_sim(A, B):
        return np.dot(A, B)/(np.linalg.norm(A)*np.linalg.norm(B))

    rec_image = {}

    for i in img_result.columns[:-1] : 
      cos = cos_sim(img_result['sentence'] , img_result[i])
      rec_image[i] = cos

    path = '/content/drive/MyDrive/비타민 컨퍼런스/images'
    n = 1
    fig = plt.figure()
    sorted_dict = sorted(rec_image.items(), key=lambda x: x[1], reverse=True)[:30]
    rand_int = np.random.choice(30,3, replace=False)

    choosed_pic = []

    for i in rand_int:
      choosed_pic.append(sorted_dict[i])

    for i in choosed_pic : 
      filename = os.path.join(path,i[0])
      img_array = img.imread(filename)
      # img_array = image.img_to_array(img_read)
      output_img = cv2.resize(img_array,dsize=(200,200),fx=0.5,fy=0.5)
      ax = fig.add_subplot(1,3,n)
      ax.imshow(output_img)
      n += 1
    
    return

sentence = "진짜 개웃기다 실화임?ㅋㅋㅋ"
recommend_images(sentence)

"""# Web"""

!pip install anvil-uplink

import anvil.server

anvil.server.connect("B725XEWFUKQMKVLLHYZPMWJ6-GPTW2UUXNFZMOCER")

from anvil.google.drive import app_files

@anvil.server.callable
def recommend_images_web(sentence) : 
    global img_result
    res = classify_sentence(sentence)
    img_result['sentence'] = res

    def cos_sim(A, B):
        return np.dot(A, B)/(np.linalg.norm(A)*np.linalg.norm(B))

    rec_image = {}

    for i in img_result.columns[:-1] : 
      cos = cos_sim(img_result['sentence'] , img_result[i])
      rec_image[i] = cos

    path = '/content/drive/MyDrive/비타민 컨퍼런스/images'

    sorted_dict = sorted(rec_image.items(), key=lambda x: x[1], reverse=True)[:30]
    rand_int = np.random.choice(30,3, replace=False)

    choosed_pic = []
    img_lst = []

    for i in rand_int:
      choosed_pic.append(sorted_dict[i])

    for i in choosed_pic : 
      filename = os.path.join(path,i[0])
      img_lst.append(i[0])

    img1, img2, img3 = img_lst
    img1_source = app_files.shared_image.get(img1)
    img2_source = app_files.shared_image.get(img2)
    img3_source = app_files.shared_image.get(img3)
    
    return img1_source, img2_source, img3_source

img1, img2, img3 = recommend_images_web(sentence)